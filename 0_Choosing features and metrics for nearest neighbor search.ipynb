{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring a large set of documents -- such as Wikipedia, news articles, StackOverflow, etc. -- it can be useful to get a list of related material. To find relevant documents you typically\n",
    "1. Decide on a notion of similarity\n",
    "* Find the documents that are most similar \n",
    "\n",
    "The goal fo this notebook is to:\n",
    "* Gain intuition for different notions of similarity and practice finding similar documents. \n",
    "* Explore the tradeoffs with representing documents using raw word counts and TF-IDF\n",
    "* Explore the behavior of different distance metrics by looking at the Wikipedia pages most similar to President Obamaâ€™s page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to Amazon EC2 users**: To conserve memory, make sure to stop all the other notebooks before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we need to first import the Python packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from scipy.sparse import csr_matrix  \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same dataset of Wikipedia pages that we used in the Machine Learning Foundations course (Course 1). Each element of the dataset consists of a link to the wikipedia article, the name of the person, and the text of the article (in lowercase).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki = pd.read_csv('people_wiki.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59071, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \n",
       "0  digby morrell born 10 october 1979 is a former...  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2  harpdog brown is a singer and harmonica player...  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(wiki.shape)\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract word count vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract word count vectors using sklearn function.  We add this as a column in `wiki`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to do word cuont\n",
    "def count_word(words):\n",
    "    counts = dict()\n",
    "    for word in words.split():\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "      <td>{u'selection': 1, u'carltons': 1, u'being': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "      <td>{u'precise': 1, u'thomas': 1, u'closely': 1, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "      <td>{u'just': 1, u'issued': 1, u'mainly': 1, u'nom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "      <td>{u'englishreading': 1, u'all': 1, u'bauforschu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "      <td>{u'own': 2, u'they': 1, u'gangstergenka': 1, u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \\\n",
       "0  digby morrell born 10 october 1979 is a former...   \n",
       "1  alfred j lewy aka sandy lewy graduated from un...   \n",
       "2  harpdog brown is a singer and harmonica player...   \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...   \n",
       "4  henry krvits born 30 december 1974 in tallinn ...   \n",
       "\n",
       "                                          word_count  \n",
       "0  {u'selection': 1, u'carltons': 1, u'being': 1,...  \n",
       "1  {u'precise': 1, u'thomas': 1, u'closely': 1, u...  \n",
       "2  {u'just': 1, u'issued': 1, u'mainly': 1, u'nom...  \n",
       "3  {u'englishreading': 1, u'all': 1, u'bauforschu...  \n",
       "4  {u'own': 2, u'they': 1, u'gangstergenka': 1, u...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use collections.Counter to do word count:\n",
    "from collections import Counter\n",
    "def count_words(x): \n",
    "    return dict(Counter(x.split()))\n",
    "\n",
    "wiki['word_count'] = map( count_words, wiki['text'])\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by finding the nearest neighbors of the Barack Obama page using the word count vectors to represent the articles and Euclidean distance to measure distance.  For this, again will we use a sklearn implementation of nearest neighbor search.\n",
    "Let's look at the top 10 nearest neighbors by performing the following query:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use CountVectorizer() to turn text to columns of word count\n",
    "One way is to use the CountVectorizer to create a sparse matrix of word count with one column for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((59071, 548429), 548429)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "wordcount_vec = vectorizer.fit_transform(wiki['text'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(wordcount_vec.shape, len(feature_names) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>33.015148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>34.307434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mitt Romney</td>\n",
       "      <td>35.791060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lawrence Summers</td>\n",
       "      <td>36.069378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Walter Mondale</td>\n",
       "      <td>36.249138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Francisco Barrio</td>\n",
       "      <td>36.276714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Don Bonker</td>\n",
       "      <td>36.400549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wynn Normington Hugh-Jones</td>\n",
       "      <td>36.441734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Refael (Rafi) Benvenisti</td>\n",
       "      <td>36.837481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Andy Anstett</td>\n",
       "      <td>36.945906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name          0\n",
       "0                 Barack Obama   0.000000\n",
       "1                    Joe Biden  33.015148\n",
       "2               George W. Bush  34.307434\n",
       "3                  Mitt Romney  35.791060\n",
       "4             Lawrence Summers  36.069378\n",
       "5               Walter Mondale  36.249138\n",
       "6             Francisco Barrio  36.276714\n",
       "7                   Don Bonker  36.400549\n",
       "8   Wynn Normington Hugh-Jones  36.441734\n",
       "9     Refael (Rafi) Benvenisti  36.837481\n",
       "10                Andy Anstett  36.945906"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "model = NearestNeighbors(metric='euclidean', algorithm='brute')\n",
    "\n",
    "OB_text = vectorizer.transform(wiki.loc[ wiki['name']=='Barack Obama', 'text'])\n",
    "distance, index = model.kneighbors(X =OB_text, n_neighbors=11) \n",
    "\n",
    "pd.concat([wiki[['name']].iloc[index[0].tolist()].reset_index().drop('index', axis=1) ,\n",
    "           pd.Series(distance[0])] , axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use DictVectorizer()  to turn 'word_count' to columns of word countÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x548561 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 273 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DV = DictVectorizer()\n",
    "wordcount_vec2 = DV.fit_transform(wiki['word_count'])\n",
    "OB_text = DV.transform(wiki.loc[ wiki['name']=='Barack Obama', 'word_count'])\n",
    "OB_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>33.075671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>34.394767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lawrence Summers</td>\n",
       "      <td>36.152455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mitt Romney</td>\n",
       "      <td>36.166283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Francisco Barrio</td>\n",
       "      <td>36.331804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Walter Mondale</td>\n",
       "      <td>36.400549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wynn Normington Hugh-Jones</td>\n",
       "      <td>36.496575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Don Bonker</td>\n",
       "      <td>36.633318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Andy Anstett</td>\n",
       "      <td>36.959437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Refael (Rafi) Benvenisti</td>\n",
       "      <td>37.202150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name          0\n",
       "0                 Barack Obama   0.000000\n",
       "1                    Joe Biden  33.075671\n",
       "2               George W. Bush  34.394767\n",
       "3             Lawrence Summers  36.152455\n",
       "4                  Mitt Romney  36.166283\n",
       "5             Francisco Barrio  36.331804\n",
       "6               Walter Mondale  36.400549\n",
       "7   Wynn Normington Hugh-Jones  36.496575\n",
       "8                   Don Bonker  36.633318\n",
       "9                 Andy Anstett  36.959437\n",
       "10    Refael (Rafi) Benvenisti  37.202150"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = NearestNeighbors( n_neighbors= 10).fit(wordcount_vec2) \n",
    "distance2, index2 = model2.kneighbors(X =OB_text, n_neighbors=11) \n",
    "\n",
    "pd.concat([wiki[['name']].iloc[index2[0].tolist()].reset_index().drop('index', axis=1) ,\n",
    "           pd.Series(distance2[0])] , axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the 10 people are politicians, but about half of them have rather tenuous connections with Obama, other than the fact that they are politicians.\n",
    "\n",
    "* Francisco Barrio is a Mexican politician, and a former governor of Chihuahua.\n",
    "* Walter Mondale and Don Bonker are Democrats who made their career in late 1970s.\n",
    "* Wynn Normington Hugh-Jones is a former British diplomat and Liberal Party official.\n",
    "* Andy Anstett is a former politician in Manitoba, Canada.\n",
    "\n",
    "Nearest neighbors with raw word counts got some things right, showing all politicians in the query result, but missed finer and important details.\n",
    "\n",
    "For instance, let's find out why Francisco Barrio was considered a close neighbor of Obama.  To do this, let's look at the most frequently used words in each of Barack Obama and Francisco Barrio's pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_words(name):\n",
    "    \"\"\"\n",
    "    Get a table of the most frequent words in the given person's wikipedia page.\n",
    "    \"\"\"\n",
    "    row = wiki[wiki['name'] == name]\n",
    "    # convert a column of dict to columns by apply(pd.Series), then transpose the tabble\n",
    "    word_count_table = row['word_count'].apply(pd.Series).transpose().reset_index()\n",
    "    word_count_table.columns= ['word', 'count']\n",
    "    \n",
    "    return word_count_table.sort_values(by= 'count', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>the</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>in</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>and</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>of</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>to</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  count\n",
       "245  the     40\n",
       "118   in     30\n",
       "31   and     21\n",
       "165   of     18\n",
       "248   to     14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_words = top_words('Barack Obama')\n",
    "obama_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>the</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>of</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>and</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>in</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>he</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  count\n",
       "204  the     36\n",
       "147   of     24\n",
       "23   and     18\n",
       "102   in     17\n",
       "95    he     10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barrio_words = top_words('Francisco Barrio')\n",
    "barrio_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the list of most frequent words that appear in both Obama's and Barrio's documents. We've so far sorted all words from Obama and Barrio's articles by their word frequencies. We will now use a dataframe operation known as **join**. The **join** operation is very useful when it comes to playing around with data: it lets you combine the content of two tables using a shared column (in this case, the word column). See [the documentation](https://dato.com/products/create/docs/generated/graphlab.SFrame.join.html) for more details.\n",
    "\n",
    "For instance, running\n",
    "```\n",
    "obama_words.join(barrio_words, on='word')\n",
    "```\n",
    "will extract the rows from both tables that correspond to the common words.\n",
    "So to obtain, say, the five common words that appear most often in Obama's article, sort the combined table by the Obama column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Obama</th>\n",
       "      <th>Barrio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  Obama  Barrio\n",
       "0  the     40      36\n",
       "1   in     30      17\n",
       "2  and     21      18\n",
       "3   of     18      24\n",
       "4   to     14       9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_words = pd.merge(left= obama_words, right = barrio_words, how= 'inner', left_on='word', right_on='word')\n",
    "combined_words.columns= ['word','Obama','Barrio']\n",
    "combined_words = combined_words.sort_values(by= 'Obama', ascending=False)\n",
    "combined_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Among the words that appear in both Barack Obama and Francisco Barrio, take the 5 that appear most frequently in Obama. How many of the articles in the Wikipedia dataset contain all of those 5 words?\n",
    "\n",
    "Hint:\n",
    "* Refer to the previous paragraph for finding the words that appear in both articles. Sort the common words by their frequencies in Obama's article and take the largest five.\n",
    "* Each word count vector is a Python dictionary. For each word count vector in SFrame, you'd have to check if the set of the 5 common words is a subset of the keys of the word count vector. Complete the function `has_top_words` to accomplish the task.\n",
    "  - Convert the list of top 5 words into set using the syntax\n",
    "```\n",
    "set(common_words)\n",
    "```\n",
    "    where `common_words` is a Python list. See [this link](https://docs.python.org/2/library/stdtypes.html#set) if you're curious about Python sets.\n",
    "  - Extract the list of keys of the word count dictionary by calling the [`keys()` method](https://docs.python.org/2/library/stdtypes.html#dict.keys).\n",
    "  - Convert the list of keys into a set as well.\n",
    "  - Use [`issubset()` method](https://docs.python.org/2/library/stdtypes.html#set) to check if all 5 words are among the keys.\n",
    "* Now apply the `has_top_words` function on every row of the DataFrame.\n",
    "* Compute the sum of the result column to obtain the number of articles containing all the 5 top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2     True\n",
       "3     True\n",
       "4    False\n",
       "Name: has_top_words, dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = combined_words['word'].head(5).tolist()\n",
    "\n",
    "def has_top_words(word_count_vector):\n",
    "    # extract the keys of word_count_vector and convert it to a set\n",
    "    unique_words = word_count_vector.keys()   # YOUR CODE HERE\n",
    "    # return True if common_words is a subset of unique_words\n",
    "    # return False otherwise\n",
    "    \n",
    "    return True if set(common_words).issubset(unique_words) else False   \n",
    "    #return True if set(common_words)<= set(unique_words) else False   \n",
    "\n",
    "wiki['has_top_words'] = wiki['word_count'].apply(has_top_words)\n",
    "\n",
    "# use has_top_words column to answer the quiz question\n",
    "print(sum(wiki['has_top_words'] ) )\n",
    "wiki['has_top_words'][:5] # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. Check your `has_top_words` function on two random articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from your function: True\n",
      "Correct output: True\n",
      "Also check the length of unique_words. It should be 167\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "print 'Output from your function:', has_top_words(wiki.loc[32,'word_count'])\n",
    "print 'Correct output: True'\n",
    "print 'Also check the length of unique_words. It should be 167'\n",
    "print( len( wiki.loc[32,'word_count'].keys()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from your function: False\n",
      "Correct output: False\n",
      "Also check the length of unique_words. It should be 188\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "print 'Output from your function:', has_top_words(wiki.loc[33,'word_count'])\n",
    "print 'Correct output: False'\n",
    "print 'Also check the length of unique_words. It should be 188'\n",
    "print( len( wiki.loc[33,'word_count'].keys()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Measure the pairwise distance between the Wikipedia pages of Barack Obama, George W. Bush, and Joe Biden. Which of the three pairs has the smallest distance?\n",
    "\n",
    "Hint: To compute the Euclidean distance between two dictionaries, use `graphlab.toolkits.distances.euclidean`. Refer to [this link](https://dato.com/products/create/docs/generated/graphlab.toolkits.distances.euclidean.html) for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 34.39476704]]\n",
      "[[ 33.07567082]]\n",
      "[[ 32.75667871]]\n"
     ]
    }
   ],
   "source": [
    "OB_text    = DV.transform(wiki.loc[ wiki['name']=='Barack Obama', 'word_count'])\n",
    "bush_text  = DV.transform(wiki.loc[ wiki['name']=='George W. Bush', 'word_count'])\n",
    "biden_text =  DV.transform(wiki.loc[ wiki['name']=='Joe Biden', 'word_count'])\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "print( pairwise_distances ( OB_text , bush_text, metric='euclidean') )\n",
    "print( pairwise_distances ( OB_text , biden_text, metric='euclidean') )\n",
    "print( pairwise_distances ( biden_text , bush_text, metric='euclidean') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Collect all words that appear both in Barack Obama and George W. Bush pages.  Out of those words, find the 10 words that show up most often in Obama's page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Obama</th>\n",
       "      <th>Bush</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>his</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>act</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>as</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  Obama  Bush\n",
       "0  the     40    39\n",
       "1   in     30    22\n",
       "2  and     21    14\n",
       "3   of     18    14\n",
       "4   to     14    11\n",
       "5  his     11     6\n",
       "6  act      8     3\n",
       "7    a      7     6\n",
       "8   he      7     8\n",
       "9   as      6     6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_words = top_words('Barack Obama')\n",
    "bush_words = top_words('George W. Bush')\n",
    "combined_words = pd.merge(left= obama_words, right = bush_words, how= 'inner', left_on='word', right_on='word')\n",
    "combined_words.columns= ['word','Obama','Bush']\n",
    "combined_words = combined_words.sort_values(by= 'Obama', ascending=False)\n",
    "combined_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note.** Even though common words are swamping out important subtle differences, commonalities in rarer political words still matter on the margin. This is why politicians are being listed in the query result instead of musicians, for example. In the next subsection, we will introduce a different metric that will place greater emphasis on those rarer words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat using precomputed wordcount vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    \n",
    "    return csr_matrix( (data, indices, indptr), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = load_sparse_csr('people_wiki_word_count.npz')\n",
    "tf_idf = load_sparse_csr('people_wiki_tf_idf.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biennials',\n",
       " 'murwara',\n",
       " 'shatzky',\n",
       " 'woode',\n",
       " 'damfunk',\n",
       " 'nualart',\n",
       " 'potentateswho',\n",
       " 'missionborn',\n",
       " 'yeardescribed',\n",
       " 'theoryhe',\n",
       " 'vinalop',\n",
       " 'soestdijk',\n",
       " 'boncea',\n",
       " 'spiders',\n",
       " 'bienniale',\n",
       " 'woody',\n",
       " 'trawling',\n",
       " 'pampoulovawagner',\n",
       " 'cubasince',\n",
       " 'laserbased',\n",
       " 'caner',\n",
       " 'canes',\n",
       " 'canet',\n",
       " 'iaspark',\n",
       " 'categoriesborn',\n",
       " '5982',\n",
       " 'caney',\n",
       " 'phosphorushe',\n",
       " 'yusaf',\n",
       " 'hhsoffice',\n",
       " '5985',\n",
       " 'gothicmade',\n",
       " 'caned',\n",
       " 'sencovici',\n",
       " 'iguau',\n",
       " 'storiesin',\n",
       " 'braziljorge',\n",
       " 'iguaz',\n",
       " 'canek',\n",
       " 'canem',\n",
       " 'victorialooking',\n",
       " 'zaikoa',\n",
       " 'racingteam',\n",
       " 'sowell',\n",
       " 'weiskopfs',\n",
       " 'ninjazz',\n",
       " 'andersanders',\n",
       " 'replacer',\n",
       " 'steppaillen',\n",
       " 'fjate',\n",
       " 'heroesin',\n",
       " 'papanastassiouwasserburgs',\n",
       " 'voiceactress',\n",
       " 'fullblooded',\n",
       " '723nnings',\n",
       " 'barcelonasp',\n",
       " 'genocidekamuhanda',\n",
       " 'radkes',\n",
       " 'composertippett',\n",
       " 'grueling',\n",
       " 'season1bouyer',\n",
       " 'wooden',\n",
       " 'comicborn',\n",
       " 'virtuosos',\n",
       " 'remainedin',\n",
       " 'illinoisrockefellersilvia',\n",
       " 'woods',\n",
       " 'woodes',\n",
       " 'townstephen',\n",
       " '598m',\n",
       " 'immunities',\n",
       " 'racedue',\n",
       " 'jher',\n",
       " 'oropolitics',\n",
       " 'thrace',\n",
       " 'voiceguitar',\n",
       " 'jhey',\n",
       " 'snuggles',\n",
       " 'brockiehe',\n",
       " 'pinerichland',\n",
       " 'deadheads',\n",
       " 'troycale',\n",
       " 'kqrsminneapolis',\n",
       " 'collegerozanskis',\n",
       " 'consenting',\n",
       " 'finalsst',\n",
       " 'auctionsin',\n",
       " 'irelandroche',\n",
       " 'career1996',\n",
       " 'leopardis',\n",
       " 'antisandinista',\n",
       " 'outfieldera',\n",
       " 'brmc',\n",
       " 'bemeriki',\n",
       " '2005marino',\n",
       " 'yanhai',\n",
       " 'termbarro',\n",
       " 'outfielders',\n",
       " 'isbin',\n",
       " 'paparizu',\n",
       " 'rocqui',\n",
       " 'alwan',\n",
       " 'bhubaneshwar',\n",
       " 'syndromehe',\n",
       " 'dogscott',\n",
       " 'slubena',\n",
       " 'iraq20042005',\n",
       " 'ninefight',\n",
       " 'canadaday',\n",
       " 'instituteoue',\n",
       " 'ulpio',\n",
       " 'convy',\n",
       " 'shvegerins',\n",
       " '2008hilary',\n",
       " 'jlana',\n",
       " 'votesprior',\n",
       " 'geoghegans',\n",
       " 'gaspatilla',\n",
       " 'deferring',\n",
       " 'gayathri',\n",
       " 'hcs',\n",
       " 'alternatingly',\n",
       " 'soundsdrew',\n",
       " 'crowdpleasing',\n",
       " '2003radloff',\n",
       " 'tokmok',\n",
       " 'vr51',\n",
       " 'talba',\n",
       " 'rebel',\n",
       " 'gastein',\n",
       " 'nok10',\n",
       " 'adultsas',\n",
       " 'gainsbourg',\n",
       " 'gasteig',\n",
       " 'legislaturebarraga',\n",
       " 'realbeanz',\n",
       " 'aviezer',\n",
       " 'lohren',\n",
       " '2004ballantine',\n",
       " 'gasteiz',\n",
       " 'chanthaburi',\n",
       " 'gurinders',\n",
       " 'hiranuma',\n",
       " 'urbanbaby',\n",
       " 'modurn',\n",
       " 'elfvin',\n",
       " 'dnd',\n",
       " 'dng',\n",
       " 'dnf',\n",
       " 'dna',\n",
       " 'englandlifsons',\n",
       " 'dnc',\n",
       " 'dnb',\n",
       " 'dnl',\n",
       " 'dno',\n",
       " 'luchin',\n",
       " 'dni',\n",
       " 'dnk',\n",
       " 'dnu',\n",
       " 'dnt',\n",
       " 'dnv',\n",
       " 'dnp',\n",
       " '320506',\n",
       " '124pm',\n",
       " 'kdziora',\n",
       " 'attenboroughin',\n",
       " 'dny',\n",
       " 'blasss',\n",
       " 'benedikt',\n",
       " 'gelbers',\n",
       " 'sonhawick',\n",
       " 'ancastedundasflamboroughwestdale',\n",
       " 'borstal',\n",
       " 'populations',\n",
       " 'chapterinvited',\n",
       " 'borstad',\n",
       " 'gijs',\n",
       " 'speakerdons',\n",
       " 'gijn',\n",
       " 'rickman',\n",
       " 'ternuraincluding',\n",
       " 'expeditionary',\n",
       " 'securitylink',\n",
       " 'biocomputers',\n",
       " 'oldwhite',\n",
       " 'ongirl',\n",
       " 'editornews',\n",
       " 'yearstandly',\n",
       " 'weaponshis',\n",
       " 'wbzfm',\n",
       " 'uhlenbeek',\n",
       " 'planprescribercom',\n",
       " 'himgould',\n",
       " 'dn5',\n",
       " 'cachaahe',\n",
       " 'godsandmonstersnet',\n",
       " 'izubuchi',\n",
       " 'studioteaterns',\n",
       " 'ozment',\n",
       " 'dangdut',\n",
       " 'zeppthe',\n",
       " 'journalsdruviete',\n",
       " 'tnnessons',\n",
       " 'legalva',\n",
       " 'irrationalist',\n",
       " 'usiuprior',\n",
       " 'mainevented',\n",
       " 'postbeginning',\n",
       " 'peopleher',\n",
       " 'langenbroich',\n",
       " 'goleas',\n",
       " 'jidaigeki',\n",
       " 'dagriculture',\n",
       " 'breedens',\n",
       " 'insectarium',\n",
       " 'masekelain',\n",
       " 'magnet2004',\n",
       " 'azurite',\n",
       " 'basolo',\n",
       " 'norplast',\n",
       " 'latifaafter',\n",
       " 'olympiadhe',\n",
       " 'andreessenhorowitz',\n",
       " 'kraig',\n",
       " 'rosmonda',\n",
       " 'venuwhich',\n",
       " 'kfrc',\n",
       " 'ncsebecause',\n",
       " 'shirked',\n",
       " 'thechancerie',\n",
       " 'tenalirama',\n",
       " 'krais',\n",
       " '1974dyson',\n",
       " 'ndvornk',\n",
       " 'managermatthews',\n",
       " 'tohnay',\n",
       " 'si850000',\n",
       " 'fieldsince',\n",
       " 'greylox',\n",
       " 'immigrantschneeberger',\n",
       " 'tevanyan',\n",
       " 'penhalonga',\n",
       " 'goober',\n",
       " 'balado',\n",
       " 'partymohamud',\n",
       " 'leavingafter',\n",
       " 'barbro',\n",
       " 'flossin',\n",
       " 'rolster',\n",
       " '2002andre',\n",
       " 'leaguesnicknamed',\n",
       " 'morsesmale',\n",
       " 'dauvergne',\n",
       " 'ombattu',\n",
       " 'varyingly',\n",
       " 'worksdadyans',\n",
       " 'arkarazo',\n",
       " 'dungkar',\n",
       " 'chandos',\n",
       " 'shreya',\n",
       " 'injurymckeever',\n",
       " 'dikinis',\n",
       " 'marquett',\n",
       " 'ontaylor',\n",
       " 'sheenboro',\n",
       " 'chandon',\n",
       " 'analysisstill',\n",
       " 'sankarabharanam',\n",
       " 'disengaging',\n",
       " 'ideman',\n",
       " 'lafource',\n",
       " 'welcomed',\n",
       " 'malba',\n",
       " 'stoicism',\n",
       " 'crossdiscipline',\n",
       " 'tillamook',\n",
       " 'tarbuckhodgson',\n",
       " 'telsys',\n",
       " 'seasoncurrie',\n",
       " 'claflin',\n",
       " 'wherein',\n",
       " 'activating',\n",
       " 'telemetryshe',\n",
       " 'michelidakidarmouslis',\n",
       " 'welcomes',\n",
       " 'unimodality',\n",
       " 'entendstu',\n",
       " 'haripal',\n",
       " 'fiq',\n",
       " 'donemusin',\n",
       " 'fit',\n",
       " 'reviewingas',\n",
       " 'powersurer',\n",
       " 'fix',\n",
       " 'nationallyaccredited',\n",
       " 'folate',\n",
       " 'fic',\n",
       " '204for815',\n",
       " 'fia',\n",
       " 'fif',\n",
       " 'fig',\n",
       " 'fid',\n",
       " 'fie',\n",
       " 'ergonomidesign',\n",
       " 'fih',\n",
       " 'fii',\n",
       " 'athleticcanvey',\n",
       " 'fio',\n",
       " 'fil',\n",
       " 'shively',\n",
       " 'characterhis',\n",
       " 'shivay',\n",
       " 'beilharz',\n",
       " 'headly',\n",
       " 'foolin',\n",
       " 'cimms',\n",
       " 'packardin',\n",
       " 'sovs',\n",
       " 'vallibel',\n",
       " '1984buffalo',\n",
       " 'hisin',\n",
       " 'mbarker',\n",
       " 'downpantsdown',\n",
       " 'challengethe',\n",
       " 'ankor',\n",
       " 'ctamong',\n",
       " 'sova',\n",
       " 'matamoras',\n",
       " 'hospitalier',\n",
       " 'tarnoczi',\n",
       " 'selaissie',\n",
       " 'scaglione',\n",
       " '26568',\n",
       " 'superdad',\n",
       " 'persita',\n",
       " 'musang',\n",
       " 'picturesfujimura',\n",
       " 'sugarplums',\n",
       " 'frewer',\n",
       " 'proffitts',\n",
       " 'grfenroda',\n",
       " 'musans',\n",
       " 'longprevailing',\n",
       " 'goverors',\n",
       " 'mancordycollins',\n",
       " 'lightningcloud',\n",
       " 'kowt',\n",
       " 'zuid',\n",
       " '1395336',\n",
       " 'victoriangeorgianinspired',\n",
       " 'pavlina',\n",
       " 'environmentallyaware',\n",
       " 'titlefrom',\n",
       " 'parasiten',\n",
       " 'associationohagan',\n",
       " 'gbowee',\n",
       " 'pisot',\n",
       " 'holisticallyhistorical',\n",
       " 'niederst',\n",
       " 'buttercups',\n",
       " 'programmesmith',\n",
       " 'ideograph',\n",
       " 'exmanchester',\n",
       " 'malczewska',\n",
       " 'roundduring',\n",
       " 'syd',\n",
       " 'combinatorial',\n",
       " 'hivaidsawareness',\n",
       " 'thioacetanilide',\n",
       " '2007jennings',\n",
       " '7in',\n",
       " 'resolvedthis',\n",
       " 'cuchera',\n",
       " 'valenciain',\n",
       " 'mansonat',\n",
       " 'cinematographyin',\n",
       " 'pusks',\n",
       " 'abbots',\n",
       " 'worldbasem',\n",
       " 'hirschmore',\n",
       " 'julide',\n",
       " 'ooooooone',\n",
       " 'bonelli',\n",
       " 'eventsakbar',\n",
       " 'pumpkins',\n",
       " 'springerkaufman',\n",
       " 'bonello',\n",
       " '19731982in',\n",
       " 'bonella',\n",
       " 'berwick',\n",
       " 'abbotm',\n",
       " 'pakmed',\n",
       " 'sikiru',\n",
       " 'danuartadanuartas',\n",
       " 'serviceexternal',\n",
       " 'husbands',\n",
       " 'mukilin',\n",
       " 'parkson',\n",
       " 'pisinski',\n",
       " '28baron',\n",
       " 'tourcoetzee',\n",
       " 'chebarkul',\n",
       " 'organisasyon',\n",
       " 'albumrolinha',\n",
       " 'kiersey',\n",
       " 'jkhis',\n",
       " 'krieble',\n",
       " 'shivajis',\n",
       " 'portmanteau',\n",
       " 'ineffectively',\n",
       " 'competitionaaltonen',\n",
       " 'kneeduring',\n",
       " 'cofounderceo',\n",
       " 'whipsaltzman',\n",
       " 'curiouser',\n",
       " 'stoneham',\n",
       " 'skanderbeg',\n",
       " 'freimanescreen',\n",
       " 'liberecagusts',\n",
       " 'hushang',\n",
       " 'jeanphi',\n",
       " 'modernitys',\n",
       " 'stylistics',\n",
       " 'analysiscurrently',\n",
       " 'niederers',\n",
       " '2014mimis',\n",
       " 'ritterband',\n",
       " 'publishingms',\n",
       " 'sarodya',\n",
       " 'aigles',\n",
       " 'cogeneral',\n",
       " '13bn',\n",
       " 'couevas',\n",
       " 'habilitandenstipendiat',\n",
       " 'capsat',\n",
       " 'swaralayam',\n",
       " 'systemsedwards',\n",
       " 'administrationscrouch',\n",
       " 'gmpls',\n",
       " '13page',\n",
       " 'shoutcasting',\n",
       " 'weighingroomriding',\n",
       " 'mukhtar',\n",
       " 'abdulwahab',\n",
       " 'izchak',\n",
       " 'casandra',\n",
       " 'bicskeis',\n",
       " 'frderpreis',\n",
       " 'rythmia',\n",
       " 'songcycles',\n",
       " '386840',\n",
       " 'thangta',\n",
       " 'yearsdrgillani',\n",
       " 'winterville',\n",
       " 'aztecadel',\n",
       " 'agurida',\n",
       " 'olthwaite',\n",
       " 'wfnyfmdave',\n",
       " 'tulsahis',\n",
       " 'talkinhe',\n",
       " '1993gormans',\n",
       " 'nonastrological',\n",
       " 'ahmet',\n",
       " 'exclaimed',\n",
       " 'schuur',\n",
       " 'mozartgemeinde',\n",
       " 'escopetarras',\n",
       " 'freelicks',\n",
       " 'performancesluisi',\n",
       " '2007lai',\n",
       " 'english2',\n",
       " 'provincecashin',\n",
       " 'hederson',\n",
       " 'trienal',\n",
       " 'nielsenstarting',\n",
       " 'germanymende',\n",
       " '7thmarch',\n",
       " '15703',\n",
       " 'eslami',\n",
       " '15706',\n",
       " '1994tunisia',\n",
       " 'mandatrio',\n",
       " 'sciencesstaffan',\n",
       " 'carlands',\n",
       " 'lilinho',\n",
       " 'merseys',\n",
       " 'vectorborne',\n",
       " 'harassmenthis',\n",
       " 'karkihis',\n",
       " '24144',\n",
       " 'cotecnas',\n",
       " 'kraftlag',\n",
       " 'healthcairncross',\n",
       " 'wewntrznych',\n",
       " 'instructionformer',\n",
       " 'trendsas',\n",
       " 'bastenesque',\n",
       " 'legislaturebeginning',\n",
       " 'welshofers',\n",
       " 'germanyk1997the',\n",
       " 'recordsoriginally',\n",
       " 'zidanegourcuff',\n",
       " 'netzbandt',\n",
       " 'footthe',\n",
       " 'jhen',\n",
       " 'kuhnian',\n",
       " 'terrorsleavitts',\n",
       " 'pixilated',\n",
       " 'karsavina',\n",
       " 'genuity',\n",
       " 'democracyrecently',\n",
       " 'existentialistsin',\n",
       " 'eurobasketcom',\n",
       " 'descosidos',\n",
       " 'trabajoin',\n",
       " '461after',\n",
       " 'reverence',\n",
       " 'caguasonce',\n",
       " 'korlalg',\n",
       " '1997bolen',\n",
       " 'modelsplatt',\n",
       " 'rocketplane',\n",
       " '2012firth',\n",
       " 'maji',\n",
       " 'majk',\n",
       " 'majd',\n",
       " 'maja',\n",
       " 'azertion',\n",
       " 'nettle',\n",
       " 'aadsonu',\n",
       " 'federley',\n",
       " 'demierre',\n",
       " 'awardkays',\n",
       " 'hiatt',\n",
       " 'hiberian',\n",
       " 'cardinalsdrafted',\n",
       " 'morawetz',\n",
       " '2012daniels',\n",
       " 'dineens',\n",
       " 'tathamlaird',\n",
       " 'dickey',\n",
       " 'katsastus',\n",
       " 'rajapaksachampika',\n",
       " 'dicker',\n",
       " 'gillham',\n",
       " 'mkhasmchog',\n",
       " 'gebrselassieworku',\n",
       " 'otello',\n",
       " 'tatsunoko',\n",
       " 'redland',\n",
       " 'belgiumaprmay',\n",
       " 'apace',\n",
       " 'topfour',\n",
       " 'economicallydriven',\n",
       " '55miles',\n",
       " 'fellowpounds',\n",
       " 'lepidoptery',\n",
       " 'meldrim',\n",
       " 'apact',\n",
       " 'kaiapoi',\n",
       " 'attacksthroughout',\n",
       " 'xlvii',\n",
       " 'bruneian',\n",
       " 'khvsgl',\n",
       " 'clubhall',\n",
       " 'christiansbill',\n",
       " '2005machage',\n",
       " 'gescom',\n",
       " 'damagede',\n",
       " 'bruneteaus',\n",
       " '2014morris',\n",
       " 'wolffin',\n",
       " 'committeeawardshe',\n",
       " 'heartsounds',\n",
       " 'festivalfor',\n",
       " 'dumontier',\n",
       " 'satalyte',\n",
       " 'beginningdeva',\n",
       " 'theodosios',\n",
       " 'eurogp',\n",
       " 'panicsville',\n",
       " 'intorno',\n",
       " 'ebooksmcmahon',\n",
       " 'shakhovskayain',\n",
       " 'worldwideyaitanes',\n",
       " 'supercoppa',\n",
       " 'ennedi',\n",
       " 'psycholinguist',\n",
       " 'thingummy',\n",
       " 'nomenclaturedickinson',\n",
       " 'researchdescribed',\n",
       " 'onewall',\n",
       " 'scotlandfraser',\n",
       " 'veenlijken',\n",
       " 'effectsmusician',\n",
       " 'dowlatabadi',\n",
       " 'classs',\n",
       " 'viehweg',\n",
       " 'aadam',\n",
       " 'primorac',\n",
       " 'nonmatchdays',\n",
       " 'programmatic',\n",
       " 'manasseris',\n",
       " 'conanp',\n",
       " 'ludacristhe',\n",
       " 'garamendi',\n",
       " 'braconi',\n",
       " 'zahlen',\n",
       " 'ncyellow',\n",
       " 'guille',\n",
       " 'tyndale',\n",
       " 'tyndall',\n",
       " 'proibidos',\n",
       " 'improvisatorysounding',\n",
       " 'doran',\n",
       " 'jorgovanka',\n",
       " 'japankrentzman',\n",
       " 'ruusunen',\n",
       " 'schofieldhe',\n",
       " 'studiesgura',\n",
       " 'particularfiguration',\n",
       " 'gizzyvan',\n",
       " 'gaventas',\n",
       " 'markpartnering',\n",
       " 'conductive',\n",
       " 'subarray',\n",
       " 'therekenneth',\n",
       " 'sunbeams',\n",
       " 'shaider',\n",
       " 'spectrometers',\n",
       " '19671dr',\n",
       " 'keyboardstony',\n",
       " 'sletten',\n",
       " 'gwozdz',\n",
       " 'seriously1',\n",
       " '2002there',\n",
       " 'lapping',\n",
       " 'bealein',\n",
       " 'bpool',\n",
       " 'gced',\n",
       " 'meridional',\n",
       " 'kazenga',\n",
       " 'cduk',\n",
       " 'copythe',\n",
       " 'gcel',\n",
       " 'understandsfollowing',\n",
       " 'mailart',\n",
       " '18921992',\n",
       " 'enochenochs',\n",
       " 'mexiconegrete',\n",
       " 'repremiered',\n",
       " 'khandap',\n",
       " 'corvair',\n",
       " 'sucarnochee',\n",
       " '2013fascinated',\n",
       " 'lishs',\n",
       " 'draughtsman',\n",
       " 'fundmcneilly',\n",
       " 'pitcheramaral',\n",
       " '2011commander',\n",
       " 'officialhis',\n",
       " 'cinepoem',\n",
       " 'honorarprofessor',\n",
       " 'fizzingly',\n",
       " 'borrussia',\n",
       " 'meaningfulness',\n",
       " 'salvagedused',\n",
       " 'lankain',\n",
       " 'gretaborn',\n",
       " 'issuesblack',\n",
       " 'ahomage',\n",
       " 'praticado',\n",
       " 'eldoradowhile',\n",
       " 'bakalyans',\n",
       " 'washingtonlaughlin',\n",
       " 'apetthassanali',\n",
       " 'otolaryngologists',\n",
       " 'escalada',\n",
       " 'castebased',\n",
       " 'dreifus',\n",
       " 'lanett',\n",
       " 'hardthrowers',\n",
       " 'cartright',\n",
       " 'icebergs',\n",
       " 'imperfecta',\n",
       " 'gnima',\n",
       " 'finsbury',\n",
       " '1670s',\n",
       " 'twangafter',\n",
       " 'berserky',\n",
       " 'giessen',\n",
       " 'takingchong',\n",
       " 'picturejournalist',\n",
       " 'baloon',\n",
       " 'sensationalized',\n",
       " 'mangual',\n",
       " 'artmahurin',\n",
       " 'miamiheadquartered',\n",
       " 'kgarm',\n",
       " 'nolanirish',\n",
       " 'koutsopoulos',\n",
       " 'arkiv',\n",
       " 'experienceboyces',\n",
       " 'state197579',\n",
       " 'jarrin',\n",
       " 'aberchirder',\n",
       " 'developmentoriented',\n",
       " 'contortion',\n",
       " 'propellants',\n",
       " 'uplifting',\n",
       " 'szetsung',\n",
       " 'teacherher',\n",
       " '4197980',\n",
       " 'australiamark',\n",
       " 'mazher',\n",
       " 'sreten',\n",
       " 'herreros',\n",
       " 'daryagang',\n",
       " 'alkhabyyr',\n",
       " 'isrioiw',\n",
       " 'jointservice',\n",
       " 'pp6176',\n",
       " 'lithuaniahe',\n",
       " 'resisprint',\n",
       " 'systemdaniel',\n",
       " 'aollynch',\n",
       " 'englandbelgium',\n",
       " 'unor',\n",
       " 'volapkspeakers',\n",
       " '2012paul',\n",
       " 'majic',\n",
       " 'canoeing',\n",
       " 'tedxtokyo',\n",
       " 'majik',\n",
       " 'instrumentshis',\n",
       " 'moviecj',\n",
       " 'unob',\n",
       " '1990playboy',\n",
       " 'bundelkhand',\n",
       " 'porvorimbbc',\n",
       " 'unog',\n",
       " 'recordsjack',\n",
       " 'branchhedgpeth',\n",
       " 'traditionskljuo',\n",
       " 'unol',\n",
       " 'kayama',\n",
       " 'fluctuationdriven',\n",
       " 'bricklayer',\n",
       " 'powsin',\n",
       " 'grinfield',\n",
       " 'planetable',\n",
       " 'remand',\n",
       " 'fouryearolds',\n",
       " 'spews',\n",
       " 'producermalavasi',\n",
       " 'valdisre',\n",
       " 'innocenceas',\n",
       " 'statefrom',\n",
       " '1998van',\n",
       " 'seasonpauls',\n",
       " 'baudry',\n",
       " 'campact',\n",
       " 'fribo',\n",
       " 'georgics',\n",
       " 'teuschl',\n",
       " 'edelstammolin',\n",
       " 'farmtomarket',\n",
       " 'georgica',\n",
       " 'zeor',\n",
       " '2006mcnamara',\n",
       " 'soundsation',\n",
       " '75in',\n",
       " 'peremptory',\n",
       " 'yearamong',\n",
       " 'menwan',\n",
       " 'mentors',\n",
       " 'statesjen',\n",
       " 'stillness',\n",
       " 'mentorm',\n",
       " 'spectacolors',\n",
       " 'rajastanafter',\n",
       " 'scratchhe',\n",
       " 'aldhousegreens',\n",
       " 'toshizumi',\n",
       " 'yorklisted',\n",
       " 'absurdities',\n",
       " '2010marlton',\n",
       " 'iwoye',\n",
       " 'programinvited',\n",
       " 'karogshe',\n",
       " 'solesides',\n",
       " 'historicalfiction',\n",
       " 'hrfest',\n",
       " 'preparando',\n",
       " 'topography',\n",
       " 'structuredcolor',\n",
       " 'olabisi',\n",
       " 'friba',\n",
       " 'andryczuk',\n",
       " 'vivalma',\n",
       " 'marignane',\n",
       " 'hastons',\n",
       " 'pastchairman',\n",
       " 'radzinsky',\n",
       " 'nationalwhitbread',\n",
       " 'grotowskis',\n",
       " 'bravewell',\n",
       " 'wleaguehe',\n",
       " 'godlin',\n",
       " '1958ehrenberg',\n",
       " 'stutzmann',\n",
       " 'bemsha',\n",
       " 'accountsshe',\n",
       " 'godlia',\n",
       " 'brays',\n",
       " 'identityhis',\n",
       " 'senecal',\n",
       " 'rescaled',\n",
       " 'dokka',\n",
       " 'yukawa',\n",
       " 'skarbfollowing',\n",
       " 'moleskine',\n",
       " 'brebants',\n",
       " 'siropulo',\n",
       " 'gligori',\n",
       " '47collingwood',\n",
       " 'championsthus',\n",
       " 'hahnville',\n",
       " 'indicative',\n",
       " 'fsicas',\n",
       " 'clevedon',\n",
       " 'saucelilker',\n",
       " 'ippnw',\n",
       " 'nonconsecutive',\n",
       " 'sleuthing',\n",
       " 'cambridgelord',\n",
       " 'bambos',\n",
       " 'languagenayler',\n",
       " '2002gerlachs',\n",
       " 'umbargers',\n",
       " 'francedeveloped',\n",
       " 'locumba',\n",
       " 'furtsch',\n",
       " 'partypauken',\n",
       " 'potterof',\n",
       " 'anklehinton',\n",
       " 'lakembafurolo',\n",
       " 'gazettephilipps',\n",
       " '2004gronkiewiczwaltz',\n",
       " 'caredata',\n",
       " 'svechi',\n",
       " 'unsinkable',\n",
       " 'rusherfollowing',\n",
       " 'yavana',\n",
       " 'staff2013',\n",
       " '2014ole',\n",
       " 'dailyhe',\n",
       " 'ecoterrorismhardwick',\n",
       " 'pileggi',\n",
       " 'lockdown',\n",
       " 'hoysince',\n",
       " 'gambale',\n",
       " 'onemake',\n",
       " 'naurai',\n",
       " 'ratatattat',\n",
       " 'fruh',\n",
       " 'chevroncorp',\n",
       " 'fruc',\n",
       " 'gamblingaddicted',\n",
       " 'mchughhis',\n",
       " 'mayor2bye',\n",
       " 'youtubeshahrzad',\n",
       " 'frud',\n",
       " 'dne',\n",
       " 'iheanacho',\n",
       " 'olegovich',\n",
       " 'itsuwa',\n",
       " 'giannabest',\n",
       " 'httpwwwhavchrengcroatianfilmcroatianfilmcataloguenosleepwontkillyou',\n",
       " 'agassi',\n",
       " 'schrder',\n",
       " 'taifai',\n",
       " 'spectatorhe',\n",
       " 'herrin',\n",
       " '1961wirsum',\n",
       " 'barracano',\n",
       " 'holofernes',\n",
       " 'hammersberg',\n",
       " 'vrilissia',\n",
       " 'qaralov',\n",
       " 'herria',\n",
       " 'fiveway',\n",
       " 'myfontscommarshs',\n",
       " 'fingleson',\n",
       " 'spaceships',\n",
       " 'svipdagr',\n",
       " 'laseriumdryer',\n",
       " 'anomalous',\n",
       " 'kopanang',\n",
       " 'biocharacterization',\n",
       " 'everestjbf',\n",
       " 'categoryluckey',\n",
       " 'wankelengined',\n",
       " 'parliamentfor',\n",
       " 'vonage',\n",
       " 'semp',\n",
       " 'honeymood',\n",
       " 'hurdlesand',\n",
       " 'associationaside',\n",
       " 'functiongrier',\n",
       " 'macedoniahis',\n",
       " 'knockdownin',\n",
       " 'marshall',\n",
       " 'honeymoon',\n",
       " 'lebedlebedbizstafflebed',\n",
       " 'oleksyduring',\n",
       " '1980s1990s',\n",
       " 'duccia',\n",
       " 'oppressionhe',\n",
       " 'cruisewagner',\n",
       " 'marshals',\n",
       " 'penaltiesdixie',\n",
       " 'haboush',\n",
       " 'university3',\n",
       " 'university2',\n",
       " 'university1',\n",
       " 'vidyanikethan',\n",
       " 'university6',\n",
       " 'university5',\n",
       " 'fluegelman',\n",
       " 'abdulhamid',\n",
       " 'biddulph',\n",
       " 'littenstall',\n",
       " 'timewitt',\n",
       " 'courtprofessor',\n",
       " 'sanremoin',\n",
       " 'digitalvibe',\n",
       " '2008gee',\n",
       " 'allsoutheastern',\n",
       " 'alexandrian',\n",
       " '1998millard',\n",
       " 'secretarykeith',\n",
       " 'dns',\n",
       " '1986persons',\n",
       " 'universityb',\n",
       " 'universitya',\n",
       " 'masterkris',\n",
       " 'tobagomaraj',\n",
       " 'thesaurus',\n",
       " 'alistairs',\n",
       " 'annemartijntje',\n",
       " 'germanirish',\n",
       " 'olowo',\n",
       " 'kaizers',\n",
       " 'moraima',\n",
       " 'universitys',\n",
       " 'vietnamese',\n",
       " 'endocytic',\n",
       " 'mgab',\n",
       " 'huoche',\n",
       " 'attenboroughii',\n",
       " 'layin',\n",
       " 'leaguegul',\n",
       " 'isku',\n",
       " 'fheasgair',\n",
       " 'mzilikazi',\n",
       " 'lesea',\n",
       " 'olubolade',\n",
       " 'poker2nite',\n",
       " 'pointreleased',\n",
       " 'allbaughs',\n",
       " 'sondelli',\n",
       " '18501950',\n",
       " '20112012a',\n",
       " 'outreaches',\n",
       " 'vflaflspalding',\n",
       " 'manasula',\n",
       " 'synchronoss',\n",
       " 'raceestabrook',\n",
       " 'scifiinflected',\n",
       " 'flipperrezabeks',\n",
       " 'brainscanhe',\n",
       " 'dannilynn',\n",
       " 'trubbel',\n",
       " 'startles',\n",
       " 'membergentles',\n",
       " 'yearssansom',\n",
       " '9781456416874',\n",
       " 'cazzetta',\n",
       " '1986032222',\n",
       " 'crowns',\n",
       " 'encontrado',\n",
       " 'facultyjerry',\n",
       " 'promotionin',\n",
       " 'carbonaceous',\n",
       " 'crowne',\n",
       " 'superdave',\n",
       " 'wordlessly',\n",
       " 'mesmerize',\n",
       " 'chairelementary',\n",
       " ...]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index_to_word = pd.read_json('people_wiki_map_index_to_word.json')[0]\n",
    "import json\n",
    "with open('people_wiki_map_index_to_word.json', 'r') as f: # Reads the list of most frequent words\n",
    "    index = json.load(f)\n",
    "map_index_to_word = [str(s) for s in index ]\n",
    "\n",
    "\n",
    "map_index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              URI          name  \\\n",
      "35817  <http://dbpedia.org/resource/Barack_Obama>  Barack Obama   \n",
      "\n",
      "                                                    text  \\\n",
      "35817  barack hussein obama ii brk husen bm born augu...   \n",
      "\n",
      "                                              word_count has_top_words  \n",
      "35817  {u'represent': 1, u'ending': 1, u'proposition'...          True  \n"
     ]
    }
   ],
   "source": [
    "model = NearestNeighbors(metric='euclidean', algorithm='brute')\n",
    "model.fit(word_count)\n",
    "print wiki[wiki['name'] == 'Barack Obama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35817</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24478</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>1.067634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28447</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>1.108710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35357</td>\n",
       "      <td>Lawrence Summers</td>\n",
       "      <td>1.116294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14754</td>\n",
       "      <td>Mitt Romney</td>\n",
       "      <td>1.139642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13229</td>\n",
       "      <td>Francisco Barrio</td>\n",
       "      <td>1.147176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31423</td>\n",
       "      <td>Walter Mondale</td>\n",
       "      <td>1.148133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22745</td>\n",
       "      <td>Wynn Normington Hugh-Jones</td>\n",
       "      <td>1.153384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36364</td>\n",
       "      <td>Don Bonker</td>\n",
       "      <td>1.157094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9210</td>\n",
       "      <td>Andy Anstett</td>\n",
       "      <td>1.159278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57145</td>\n",
       "      <td>Emil Skodon</td>\n",
       "      <td>1.159457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        name  distance\n",
       "0   35817                Barack Obama  0.000000\n",
       "1   24478                   Joe Biden  1.067634\n",
       "2   28447              George W. Bush  1.108710\n",
       "3   35357            Lawrence Summers  1.116294\n",
       "4   14754                 Mitt Romney  1.139642\n",
       "5   13229            Francisco Barrio  1.147176\n",
       "6   31423              Walter Mondale  1.148133\n",
       "7   22745  Wynn Normington Hugh-Jones  1.153384\n",
       "8   36364                  Don Bonker  1.157094\n",
       "9    9210                Andy Anstett  1.159278\n",
       "10  57145                 Emil Skodon  1.159457"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = model.kneighbors(word_count[35817], n_neighbors=11) # 1st arg: word count vector\n",
    "tenNN = pd.concat([pd.Series(indices[0].tolist()), wiki[['name']].iloc[indices[0].tolist()].reset_index().drop('index', axis=1) ,\n",
    "           pd.Series(distance[0])] , axis=1 )\n",
    "tenNN.columns = ['id', 'name', 'distance']\n",
    "tenNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the following cell to obtain the word_count column, which represents the word count vectors in the dictionary form. This way, we can quickly recognize words of great importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF to the rescue\n",
    "\n",
    "Much of the perceived commonalities between Obama and Barrio were due to occurrences of extremely frequent words, such as \"the\", \"and\", and \"his\". So nearest neighbors is recommending plausible results sometimes for the wrong reasons. \n",
    "\n",
    "To retrieve articles that are more relevant, we should focus more on rare words that don't happen in every article. **TF-IDF** (term frequencyâ€“inverse document frequency) is a feature representation that penalizes words that are too common.  Let's use GraphLab Create's implementation of TF-IDF and repeat the search for the 10 nearest neighbors of Barack Obama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "dict_vec = DictVectorizer()\n",
    "\n",
    "tfidf.fit_transform(dict_vec.fit_transform( wiki['word_count']) )\n",
    "\n",
    "OB_text = tfidf.transform(dict_vec.transform( \n",
    "    wiki.loc[wiki['name']== 'Barack Obama', 'word_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>1.067634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Rodham Clinton</td>\n",
       "      <td>1.108710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Samantha Power</td>\n",
       "      <td>1.116294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric Stern (politician)</td>\n",
       "      <td>1.139642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>1.147176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>John McCain</td>\n",
       "      <td>1.148133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Artur Davis</td>\n",
       "      <td>1.153384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Henry Waxman</td>\n",
       "      <td>1.157094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jeff Sessions</td>\n",
       "      <td>1.159278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name         0\n",
       "0             Barack Obama  0.000000\n",
       "1                Joe Biden  1.067634\n",
       "2   Hillary Rodham Clinton  1.108710\n",
       "3           Samantha Power  1.116294\n",
       "4  Eric Stern (politician)  1.139642\n",
       "5           George W. Bush  1.147176\n",
       "6              John McCain  1.148133\n",
       "7              Artur Davis  1.153384\n",
       "8             Henry Waxman  1.157094\n",
       "9            Jeff Sessions  1.159278"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf_idf =  NearestNeighbors(metric='euclidean', algorithm='brute')\n",
    "model_tf_idf.fit(tfidf_vec) \n",
    "\n",
    "distance2, index2 = model_tf_idf.kneighbors(X =OB_text, n_neighbors=10) \n",
    "\n",
    "pd.concat([wiki[['name']].iloc[index2[0].tolist()].reset_index().drop('index', axis=1) ,\n",
    "           pd.Series(distance2[0])] , axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat using pre-computed tfidf scores to match assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phil Schiliro</td>\n",
       "      <td>106.861014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeff Sessions</td>\n",
       "      <td>108.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jesse Lee (politician)</td>\n",
       "      <td>109.045698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samantha Power</td>\n",
       "      <td>109.108106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bob Menendez</td>\n",
       "      <td>109.781867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eric Stern (politician)</td>\n",
       "      <td>109.957788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>James A. Guest</td>\n",
       "      <td>110.413889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Roland Grossenbacher</td>\n",
       "      <td>110.470609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tulsi Gabbard</td>\n",
       "      <td>110.696998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Howard Dawson</td>\n",
       "      <td>110.730547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name           0\n",
       "0              Barack Obama    0.000000\n",
       "1             Phil Schiliro  106.861014\n",
       "2             Jeff Sessions  108.871674\n",
       "3    Jesse Lee (politician)  109.045698\n",
       "4            Samantha Power  109.108106\n",
       "5              Bob Menendez  109.781867\n",
       "6   Eric Stern (politician)  109.957788\n",
       "7            James A. Guest  110.413889\n",
       "8      Roland Grossenbacher  110.470609\n",
       "9             Tulsi Gabbard  110.696998\n",
       "10            Howard Dawson  110.730547"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OB_index = wiki[wiki['name']== 'Barack Obama'].index.tolist()[0]\n",
    "\n",
    "model_tf_idf =  NearestNeighbors(metric='euclidean', algorithm='brute')\n",
    "model_tf_idf.fit(tf_idf) \n",
    "\n",
    "distance2, index2 = model_tf_idf.kneighbors(X = tf_idf[OB_index], \n",
    "                                            n_neighbors=11) \n",
    "\n",
    "pd.concat([wiki[['name']].iloc[index2[0].tolist()].reset_index().drop('index', axis=1) ,\n",
    "           pd.Series(distance2[0])] , axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine whether this list makes sense.\n",
    "* With a notable exception of Roland Grossenbacher, the other 8 are all American politicians who are contemporaries of Barack Obama.\n",
    "* Phil Schiliro, Jesse Lee, Samantha Power, and Eric Stern worked for Obama.\n",
    "\n",
    "Clearly, the results are more plausible with the use of TF-IDF. Let's take a look at the word vector for Obama and Schilirio's pages. Notice that TF-IDF representation assigns a weight to each word. This weight captures relative importance of that word in the document. Let us sort the words in Obama's article by their TF-IDF weights; we do the same for Schiliro's article as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32093</th>\n",
       "      <td>blobs</td>\n",
       "      <td>10.986495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103658</th>\n",
       "      <td>fisborn</td>\n",
       "      <td>10.986495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120780</th>\n",
       "      <td>polyrhythm</td>\n",
       "      <td>10.986495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397642</th>\n",
       "      <td>sheene</td>\n",
       "      <td>10.293348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409594</th>\n",
       "      <td>afroasiatic</td>\n",
       "      <td>10.293348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452469</th>\n",
       "      <td>bernelli</td>\n",
       "      <td>9.887883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492548</th>\n",
       "      <td>sugdensmith</td>\n",
       "      <td>9.040585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499835</th>\n",
       "      <td>tritium</td>\n",
       "      <td>8.907054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512805</th>\n",
       "      <td>systemwhen</td>\n",
       "      <td>8.421546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526389</th>\n",
       "      <td>disappoint</td>\n",
       "      <td>7.690659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526900</th>\n",
       "      <td>maltempo</td>\n",
       "      <td>7.654291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529331</th>\n",
       "      <td>lamain</td>\n",
       "      <td>7.460135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529495</th>\n",
       "      <td>oneover</td>\n",
       "      <td>7.431147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531329</th>\n",
       "      <td>canadabesides</td>\n",
       "      <td>7.297616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531869</th>\n",
       "      <td>njin</td>\n",
       "      <td>7.248826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532290</th>\n",
       "      <td>httpwwwcwchforg</td>\n",
       "      <td>7.202306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533599</th>\n",
       "      <td>govilon</td>\n",
       "      <td>7.074472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534868</th>\n",
       "      <td>bibliotecas</td>\n",
       "      <td>6.926052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535658</th>\n",
       "      <td>litwiski</td>\n",
       "      <td>6.843361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535765</th>\n",
       "      <td>perlungher</td>\n",
       "      <td>6.827612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535818</th>\n",
       "      <td>mditerrane</td>\n",
       "      <td>6.812108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536238</th>\n",
       "      <td>2012details</td>\n",
       "      <td>6.766988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536530</th>\n",
       "      <td>huntglobalahipara</td>\n",
       "      <td>6.723816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536610</th>\n",
       "      <td>partyreimer</td>\n",
       "      <td>6.709829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536900</th>\n",
       "      <td>creamier</td>\n",
       "      <td>6.669007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537075</th>\n",
       "      <td>dillonhayes</td>\n",
       "      <td>6.642690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537130</th>\n",
       "      <td>convinced</td>\n",
       "      <td>6.642690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537367</th>\n",
       "      <td>fenosa</td>\n",
       "      <td>6.604469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538506</th>\n",
       "      <td>ebell</td>\n",
       "      <td>6.432618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539135</th>\n",
       "      <td>manasseh</td>\n",
       "      <td>6.332535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547943</th>\n",
       "      <td>gat</td>\n",
       "      <td>2.372186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547945</th>\n",
       "      <td>bruschi</td>\n",
       "      <td>3.382133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547946</th>\n",
       "      <td>shyatt</td>\n",
       "      <td>1.098883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547947</th>\n",
       "      <td>2002stone</td>\n",
       "      <td>1.089076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547948</th>\n",
       "      <td>ratestogocom</td>\n",
       "      <td>1.075238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547950</th>\n",
       "      <td>2007keim</td>\n",
       "      <td>3.777334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547953</th>\n",
       "      <td>episodesamong</td>\n",
       "      <td>0.887153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547954</th>\n",
       "      <td>fiminielsen</td>\n",
       "      <td>1.694686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547955</th>\n",
       "      <td>nonpartner</td>\n",
       "      <td>0.767431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547956</th>\n",
       "      <td>jonsey</td>\n",
       "      <td>2.086815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547957</th>\n",
       "      <td>delhichhibber</td>\n",
       "      <td>0.661407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547959</th>\n",
       "      <td>legislaturedempsey</td>\n",
       "      <td>1.713990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547960</th>\n",
       "      <td>torpey</td>\n",
       "      <td>0.374553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547962</th>\n",
       "      <td>18b</td>\n",
       "      <td>0.881266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547963</th>\n",
       "      <td>ipfw</td>\n",
       "      <td>0.536393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547964</th>\n",
       "      <td>vflathorne</td>\n",
       "      <td>2.888726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547965</th>\n",
       "      <td>gan</td>\n",
       "      <td>0.430639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547966</th>\n",
       "      <td>tieon</td>\n",
       "      <td>1.493580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547967</th>\n",
       "      <td>anime</td>\n",
       "      <td>0.607406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547968</th>\n",
       "      <td>shawnees</td>\n",
       "      <td>0.368826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547969</th>\n",
       "      <td>morihiro</td>\n",
       "      <td>0.763017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547970</th>\n",
       "      <td>anglique</td>\n",
       "      <td>0.396829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547971</th>\n",
       "      <td>229326</td>\n",
       "      <td>0.291450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547972</th>\n",
       "      <td>sptar</td>\n",
       "      <td>0.657229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547973</th>\n",
       "      <td>4week</td>\n",
       "      <td>0.055233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547974</th>\n",
       "      <td>bergey</td>\n",
       "      <td>0.039334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547975</th>\n",
       "      <td>magazineone</td>\n",
       "      <td>0.074811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547976</th>\n",
       "      <td>feirberg</td>\n",
       "      <td>0.028962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547977</th>\n",
       "      <td>sussexvisiting</td>\n",
       "      <td>0.015648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547978</th>\n",
       "      <td>zinka</td>\n",
       "      <td>0.004063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>273 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     index          0\n",
       "32093                blobs  10.986495\n",
       "103658             fisborn  10.986495\n",
       "120780          polyrhythm  10.986495\n",
       "397642              sheene  10.293348\n",
       "409594         afroasiatic  10.293348\n",
       "452469            bernelli   9.887883\n",
       "492548         sugdensmith   9.040585\n",
       "499835             tritium   8.907054\n",
       "512805          systemwhen   8.421546\n",
       "526389          disappoint   7.690659\n",
       "526900            maltempo   7.654291\n",
       "529331              lamain   7.460135\n",
       "529495             oneover   7.431147\n",
       "531329       canadabesides   7.297616\n",
       "531869                njin   7.248826\n",
       "532290     httpwwwcwchforg   7.202306\n",
       "533599             govilon   7.074472\n",
       "534868         bibliotecas   6.926052\n",
       "535658            litwiski   6.843361\n",
       "535765          perlungher   6.827612\n",
       "535818          mditerrane   6.812108\n",
       "536238         2012details   6.766988\n",
       "536530   huntglobalahipara   6.723816\n",
       "536610         partyreimer   6.709829\n",
       "536900            creamier   6.669007\n",
       "537075         dillonhayes   6.642690\n",
       "537130           convinced   6.642690\n",
       "537367              fenosa   6.604469\n",
       "538506               ebell   6.432618\n",
       "539135            manasseh   6.332535\n",
       "...                    ...        ...\n",
       "547943                 gat   2.372186\n",
       "547945             bruschi   3.382133\n",
       "547946              shyatt   1.098883\n",
       "547947           2002stone   1.089076\n",
       "547948        ratestogocom   1.075238\n",
       "547950            2007keim   3.777334\n",
       "547953       episodesamong   0.887153\n",
       "547954         fiminielsen   1.694686\n",
       "547955          nonpartner   0.767431\n",
       "547956              jonsey   2.086815\n",
       "547957       delhichhibber   0.661407\n",
       "547959  legislaturedempsey   1.713990\n",
       "547960              torpey   0.374553\n",
       "547962                 18b   0.881266\n",
       "547963                ipfw   0.536393\n",
       "547964          vflathorne   2.888726\n",
       "547965                 gan   0.430639\n",
       "547966               tieon   1.493580\n",
       "547967               anime   0.607406\n",
       "547968            shawnees   0.368826\n",
       "547969            morihiro   0.763017\n",
       "547970            anglique   0.396829\n",
       "547971              229326   0.291450\n",
       "547972               sptar   0.657229\n",
       "547973               4week   0.055233\n",
       "547974              bergey   0.039334\n",
       "547975         magazineone   0.074811\n",
       "547976            feirberg   0.028962\n",
       "547977      sussexvisiting   0.015648\n",
       "547978               zinka   0.004063\n",
       "\n",
       "[273 rows x 2 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_index = wiki[wiki['name'] == 'Barack Obama'].index.tolist()[0]\n",
    "tfidf_data = pd.DataFrame(data = tf_idf[row_index].toarray().transpose(), \n",
    "                             index= map_index_to_word ).reset_index()\n",
    "tfidf_data[tfidf_data[0] >0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_words_tf_idf(name):\n",
    "    \"\"\"\n",
    "    Get a table of the most frequent words in the given person's wikipedia page.\n",
    "    \"\"\"\n",
    "    row_index = wiki[wiki['name'] == name].index.tolist()[0]\n",
    "    tfidf_data = pd.DataFrame(data = tf_idf[row_index].toarray().transpose(), \n",
    "                             index= map_index_to_word ).reset_index()\n",
    "    tfidf_data.columns=['word', 'weight']\n",
    "    return tfidf_data[tfidf_data['weight']> 0].sort_values(by= 'weight', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         word     weight\n",
      "544962                nordahl  43.295653\n",
      "547084          murderforhire  27.678223\n",
      "545808                 allows  17.747379\n",
      "546835    politicianvallarino  14.887061\n",
      "547685                branwyn  14.722936\n",
      "544887                 alytus  14.533374\n",
      "547233                   nuis  13.115933\n",
      "546139               1960gunn  12.784385\n",
      "546136             verwaltung  12.784385\n",
      "547347                alfonse  12.410689\n",
      "547842           oncehallberg  11.591943\n",
      "103658                fisborn  10.986495\n",
      "32093                   blobs  10.986495\n",
      "120780             polyrhythm  10.986495\n",
      "397642                 sheene  10.293348\n",
      "409594            afroasiatic  10.293348\n",
      "547146              violating  10.164288\n",
      "452469               bernelli   9.887883\n",
      "545169               shuntaro   9.431014\n",
      "545187          segmentsharma   9.419704\n",
      "547344           2008although   9.319342\n",
      "547397                   deng   9.077468\n",
      "492548            sugdensmith   9.040585\n",
      "545723       directorminister   8.967411\n",
      "499835                tritium   8.907054\n",
      "545840                  saffi   8.842461\n",
      "545960              ebayclark   8.698475\n",
      "512805             systemwhen   8.421546\n",
      "546317                playsli   8.281231\n",
      "547647                 mrtons   7.712676\n",
      "...                       ...        ...\n",
      "547914              newarkers   1.496782\n",
      "547966                  tieon   1.493580\n",
      "547915  httpwwwplanetbronxcom   1.491503\n",
      "547916     integritykiplinger   1.487973\n",
      "547917        humanagentrobot   1.487823\n",
      "547921                 cumnor   1.442401\n",
      "547923                 asrama   1.430935\n",
      "547928                 surmay   1.383640\n",
      "547946                 shyatt   1.098883\n",
      "547947              2002stone   1.089076\n",
      "547948           ratestogocom   1.075238\n",
      "547953          episodesamong   0.887153\n",
      "547962                    18b   0.881266\n",
      "547955             nonpartner   0.767431\n",
      "547969               morihiro   0.763017\n",
      "547957          delhichhibber   0.661407\n",
      "547972                  sptar   0.657229\n",
      "547967                  anime   0.607406\n",
      "547963                   ipfw   0.536393\n",
      "547965                    gan   0.430639\n",
      "547970               anglique   0.396829\n",
      "547960                 torpey   0.374553\n",
      "547968               shawnees   0.368826\n",
      "547971                 229326   0.291450\n",
      "547975            magazineone   0.074811\n",
      "547973                  4week   0.055233\n",
      "547974                 bergey   0.039334\n",
      "547976               feirberg   0.028962\n",
      "547977         sussexvisiting   0.015648\n",
      "547978                  zinka   0.004063\n",
      "\n",
      "[273 rows x 2 columns]\n",
      "                                                     word     weight\n",
      "354746                                              silkk  21.972991\n",
      "547305                                     instabilityhis  15.856442\n",
      "545646                                               cove  13.547088\n",
      "249027                                           tetzlaff  10.986495\n",
      "544962                                            nordahl   9.621256\n",
      "493241                                              aafia   9.040585\n",
      "547866                                         2009bryant   9.033587\n",
      "506130                                            burtonr   8.683910\n",
      "547685                                            branwyn   7.361468\n",
      "547089                                     economicssince   6.913104\n",
      "535684                                     positionstokes   6.827612\n",
      "547185                                           hallowes   6.687943\n",
      "537186                                       brahmacharin   6.629787\n",
      "537741                                             shales   6.555679\n",
      "539365                                              23042   6.295148\n",
      "547347                                            alfonse   6.205344\n",
      "540205                                                  q   6.134465\n",
      "542506                                         bamanankan   5.629909\n",
      "547652                                    disbandedkirwan   5.104064\n",
      "544327                                     recordsstevens   5.067602\n",
      "547667                                     covergirlbrand   5.044141\n",
      "547892                                     publicationthe   4.845171\n",
      "544919                                               kenu   4.831637\n",
      "544933                                       therebrouder   4.825288\n",
      "545155                                       trojanisches   4.721194\n",
      "545268                                              cunha   4.682047\n",
      "547742                                       slandyakutia   4.651096\n",
      "545639                                 championshipsafter   4.520351\n",
      "545693                                           annelyse   4.497290\n",
      "545846                                               dmoz   4.417014\n",
      "...                                                   ...        ...\n",
      "547876                                           footwork   1.732042\n",
      "547881                                        womenhorner   1.690344\n",
      "547898                                          pointlike   1.567922\n",
      "547899                                        reapersince   1.564436\n",
      "547902                                           bystrice   1.553892\n",
      "547913                                       hirschbiegel   1.509339\n",
      "547914                                          newarkers   1.496782\n",
      "547927                                               15km   1.399364\n",
      "547938                                           guadagno   1.305027\n",
      "547940                                       murosposadas   1.227282\n",
      "547941                                           slowburn   1.222787\n",
      "547945                                            bruschi   1.127378\n",
      "547947                                          2002stone   1.089076\n",
      "547959                                 legislaturedempsey   0.856995\n",
      "547954                                        fiminielsen   0.847343\n",
      "547968                                           shawnees   0.737651\n",
      "547957                                      delhichhibber   0.661407\n",
      "547961  hangshttpwwwmoroccoworldnewscom201410141065rab...   0.596478\n",
      "547962                                                18b   0.587511\n",
      "547969                                           morihiro   0.508678\n",
      "547972                                              sptar   0.328615\n",
      "547970                                           anglique   0.317463\n",
      "547971                                             229326   0.291450\n",
      "547964                                         vflathorne   0.262611\n",
      "547973                                              4week   0.055233\n",
      "547975                                        magazineone   0.037406\n",
      "547974                                             bergey   0.028096\n",
      "547976                                           feirberg   0.004827\n",
      "547977                                     sussexvisiting   0.004471\n",
      "547978                                              zinka   0.001016\n",
      "\n",
      "[119 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "obama_tf_idf = top_words_tf_idf('Barack Obama')\n",
    "print obama_tf_idf\n",
    "\n",
    "schiliro_tf_idf = top_words_tf_idf('Phil Schiliro')\n",
    "print schiliro_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **join** operation we learned earlier, try your hands at computing the common words shared by Obama's and Schiliro's articles. Sort the common words by their TF-IDF weights in Obama's document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Obama</th>\n",
       "      <th>Schiliro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nordahl</td>\n",
       "      <td>43.295653</td>\n",
       "      <td>9.621256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>branwyn</td>\n",
       "      <td>14.722936</td>\n",
       "      <td>7.361468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alfonse</td>\n",
       "      <td>12.410689</td>\n",
       "      <td>6.205344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>violating</td>\n",
       "      <td>10.164288</td>\n",
       "      <td>3.388096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agua</td>\n",
       "      <td>7.386955</td>\n",
       "      <td>3.693478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009bryant</td>\n",
       "      <td>7.226869</td>\n",
       "      <td>9.033587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>allreds</td>\n",
       "      <td>6.095386</td>\n",
       "      <td>3.047693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007walkers</td>\n",
       "      <td>5.473201</td>\n",
       "      <td>1.824400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kalyana</td>\n",
       "      <td>5.248173</td>\n",
       "      <td>2.624086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>athenshis</td>\n",
       "      <td>5.107041</td>\n",
       "      <td>3.404694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word      Obama  Schiliro\n",
       "0      nordahl  43.295653  9.621256\n",
       "1      branwyn  14.722936  7.361468\n",
       "2      alfonse  12.410689  6.205344\n",
       "3    violating  10.164288  3.388096\n",
       "4         agua   7.386955  3.693478\n",
       "5   2009bryant   7.226869  9.033587\n",
       "6      allreds   6.095386  3.047693\n",
       "7  2007walkers   5.473201  1.824400\n",
       "8      kalyana   5.248173  2.624086\n",
       "9    athenshis   5.107041  3.404694"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_words = pd.merge(left= obama_tf_idf , right = schiliro_tf_idf, \n",
    "                          how= 'inner', left_on='word', right_on='word')\n",
    "combined_words.columns= ['word','Obama','Schiliro']\n",
    "combined_words = combined_words.sort_values(by= 'Obama', ascending=False)\n",
    "combined_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 10 words should say: Obama, law, democratic, Senate, presidential, president, policy, states, office, 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Among the words that appear in both Barack Obama and Phil Schiliro, take the 5 that have largest weights in Obama. How many of the articles in the Wikipedia dataset contain all of those 5 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tf_idf.toarray()) \n",
    "tfidf_df.columns= map_index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nordahl', 'branwyn', 'alfonse', 'violating', 'agua']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nordahl</th>\n",
       "      <th>branwyn</th>\n",
       "      <th>alfonse</th>\n",
       "      <th>violating</th>\n",
       "      <th>agua</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>9.621256</td>\n",
       "      <td>4.907645</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>3.388096</td>\n",
       "      <td>7.386955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>4.810628</td>\n",
       "      <td>2.453823</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>3.388096</td>\n",
       "      <td>7.386955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>9.621256</td>\n",
       "      <td>7.361468</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>3.388096</td>\n",
       "      <td>3.693478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7950</th>\n",
       "      <td>4.810628</td>\n",
       "      <td>7.361468</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>6.776192</td>\n",
       "      <td>3.693478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8063</th>\n",
       "      <td>4.810628</td>\n",
       "      <td>2.453823</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>10.164288</td>\n",
       "      <td>7.386955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11517</th>\n",
       "      <td>9.621256</td>\n",
       "      <td>9.815291</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>3.388096</td>\n",
       "      <td>3.693478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14754</th>\n",
       "      <td>4.810628</td>\n",
       "      <td>2.453823</td>\n",
       "      <td>3.102672</td>\n",
       "      <td>3.388096</td>\n",
       "      <td>18.467389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>4.810628</td>\n",
       "      <td>2.453823</td>\n",
       "      <td>3.102672</td>\n",
       "      <td>6.776192</td>\n",
       "      <td>3.693478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24478</th>\n",
       "      <td>19.242512</td>\n",
       "      <td>2.453823</td>\n",
       "      <td>9.308017</td>\n",
       "      <td>10.164288</td>\n",
       "      <td>7.386955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35817</th>\n",
       "      <td>43.295653</td>\n",
       "      <td>14.722936</td>\n",
       "      <td>12.410689</td>\n",
       "      <td>10.164288</td>\n",
       "      <td>7.386955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45068</th>\n",
       "      <td>9.621256</td>\n",
       "      <td>2.453823</td>\n",
       "      <td>9.308017</td>\n",
       "      <td>3.388096</td>\n",
       "      <td>3.693478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47303</th>\n",
       "      <td>4.810628</td>\n",
       "      <td>2.453823</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>23.716672</td>\n",
       "      <td>7.386955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55880</th>\n",
       "      <td>9.621256</td>\n",
       "      <td>2.453823</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>6.776192</td>\n",
       "      <td>7.386955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57108</th>\n",
       "      <td>9.621256</td>\n",
       "      <td>7.361468</td>\n",
       "      <td>6.205344</td>\n",
       "      <td>10.164288</td>\n",
       "      <td>7.386955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         nordahl    branwyn    alfonse  violating       agua\n",
       "3384    9.621256   4.907645   6.205344   3.388096   7.386955\n",
       "6896    4.810628   2.453823   6.205344   3.388096   7.386955\n",
       "7914    9.621256   7.361468   6.205344   3.388096   3.693478\n",
       "7950    4.810628   7.361468   6.205344   6.776192   3.693478\n",
       "8063    4.810628   2.453823   6.205344  10.164288   7.386955\n",
       "11517   9.621256   9.815291   6.205344   3.388096   3.693478\n",
       "14754   4.810628   2.453823   3.102672   3.388096  18.467389\n",
       "15795   4.810628   2.453823   3.102672   6.776192   3.693478\n",
       "24478  19.242512   2.453823   9.308017  10.164288   7.386955\n",
       "35817  43.295653  14.722936  12.410689  10.164288   7.386955\n",
       "45068   9.621256   2.453823   9.308017   3.388096   3.693478\n",
       "47303   4.810628   2.453823   6.205344  23.716672   7.386955\n",
       "55880   9.621256   2.453823   6.205344   6.776192   7.386955\n",
       "57108   9.621256   7.361468   6.205344  10.164288   7.386955"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(common_words)\n",
    "mat = tfidf_df[common_words]\n",
    "mat0= mat[mat[common_words[0]] >0]\n",
    "mat1= mat0[mat0[common_words[1]] >0]\n",
    "mat2= mat1[mat1[common_words[2]] >0]\n",
    "mat3= mat2[mat2[common_words[3]] >0]\n",
    "mat4= mat3[mat3[common_words[4]] >0]\n",
    "mat4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3384                 Caroline Kennedy\n",
       "6896     Jonathan Singer (journalist)\n",
       "7914                    Phil Schiliro\n",
       "7950                 Elizabeth Warren\n",
       "8063                     Sarah Sewall\n",
       "11517                    Louis Susman\n",
       "14754                     Mitt Romney\n",
       "15795                   Larry Kilgore\n",
       "24478                       Joe Biden\n",
       "35817                    Barack Obama\n",
       "45068                  Douglas Schoen\n",
       "47303                      John Kerry\n",
       "55880                      Sheila Nix\n",
       "57108          Hillary Rodham Clinton\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# names who have these top words\n",
    "wiki.loc[mat4.index.tolist(),'name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the huge difference in this calculation using TF-IDF scores instead  of raw word counts. We've eliminated noise arising from extremely common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why Joe Biden, Obama's running mate in two presidential elections, is missing from the query results of `model_tf_idf`. Let's find out why. First, compute the distance between TF-IDF features of Obama and Biden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Compute the Euclidean distance between TF-IDF features of Obama and Biden. Hint: When using Boolean filter in SFrame/SArray, take the index 0 to access the first match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Obama</th>\n",
       "      <th>Biden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nordahl</td>\n",
       "      <td>43.295653</td>\n",
       "      <td>19.242512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>murderforhire</td>\n",
       "      <td>27.678223</td>\n",
       "      <td>17.298889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allows</td>\n",
       "      <td>17.747379</td>\n",
       "      <td>4.436845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politicianvallarino</td>\n",
       "      <td>14.887061</td>\n",
       "      <td>7.443530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>branwyn</td>\n",
       "      <td>14.722936</td>\n",
       "      <td>2.453823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alytus</td>\n",
       "      <td>14.533374</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nuis</td>\n",
       "      <td>13.115933</td>\n",
       "      <td>3.278983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1960gunn</td>\n",
       "      <td>12.784385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>verwaltung</td>\n",
       "      <td>12.784385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alfonse</td>\n",
       "      <td>12.410689</td>\n",
       "      <td>9.308017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>oncehallberg</td>\n",
       "      <td>11.591943</td>\n",
       "      <td>9.659952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fisborn</td>\n",
       "      <td>10.986495</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blobs</td>\n",
       "      <td>10.986495</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polyrhythm</td>\n",
       "      <td>10.986495</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sheene</td>\n",
       "      <td>10.293348</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>afroasiatic</td>\n",
       "      <td>10.293348</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>violating</td>\n",
       "      <td>10.164288</td>\n",
       "      <td>10.164288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bernelli</td>\n",
       "      <td>9.887883</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>shuntaro</td>\n",
       "      <td>9.431014</td>\n",
       "      <td>4.715507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>segmentsharma</td>\n",
       "      <td>9.419704</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2008although</td>\n",
       "      <td>9.319342</td>\n",
       "      <td>3.106447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>deng</td>\n",
       "      <td>9.077468</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sugdensmith</td>\n",
       "      <td>9.040585</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>directorminister</td>\n",
       "      <td>8.967411</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tritium</td>\n",
       "      <td>8.907054</td>\n",
       "      <td>8.907054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>saffi</td>\n",
       "      <td>8.842461</td>\n",
       "      <td>8.842461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ebayclark</td>\n",
       "      <td>8.698475</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>systemwhen</td>\n",
       "      <td>8.421546</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>playsli</td>\n",
       "      <td>8.281231</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mrtons</td>\n",
       "      <td>7.712676</td>\n",
       "      <td>5.141784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>50cc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.660120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>convery</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.527567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>wkiewdekwrza</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.496468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>ranthambore</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.453823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>himswannack</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.449107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>previte</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.440865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>clyde</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.375084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>chearavanont</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.343022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>adrenalinin</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.339854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>sivarasa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.299053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>5153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.240256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>princeweaver</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.166978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>thoroughfare</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.144758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>exuberance</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.960920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>philippinesdisney</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.929423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>courtsother</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.875313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>33story</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.848403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>dieguito</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.759101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>homeownergardeners</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.673057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>gameknown</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.545440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>awardyearwood</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.521978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>cechova</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.429450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>spanky</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.418620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>alliancesan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.406248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>farmerhe</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.357313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>fuine</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.325334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>rolesrivka</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.313995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1973davis</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.977477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>gap</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.462727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>hangshttpwwwmoroccoworldnewscom201410141065rab...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  word      Obama      Biden\n",
       "0                                              nordahl  43.295653  19.242512\n",
       "1                                        murderforhire  27.678223  17.298889\n",
       "2                                               allows  17.747379   4.436845\n",
       "3                                  politicianvallarino  14.887061   7.443530\n",
       "4                                              branwyn  14.722936   2.453823\n",
       "5                                               alytus  14.533374   0.000000\n",
       "6                                                 nuis  13.115933   3.278983\n",
       "7                                             1960gunn  12.784385   0.000000\n",
       "8                                           verwaltung  12.784385   0.000000\n",
       "9                                              alfonse  12.410689   9.308017\n",
       "10                                        oncehallberg  11.591943   9.659952\n",
       "11                                             fisborn  10.986495   0.000000\n",
       "12                                               blobs  10.986495   0.000000\n",
       "13                                          polyrhythm  10.986495   0.000000\n",
       "14                                              sheene  10.293348   0.000000\n",
       "15                                         afroasiatic  10.293348   0.000000\n",
       "16                                           violating  10.164288  10.164288\n",
       "17                                            bernelli   9.887883   0.000000\n",
       "18                                            shuntaro   9.431014   4.715507\n",
       "19                                       segmentsharma   9.419704   0.000000\n",
       "20                                        2008although   9.319342   3.106447\n",
       "21                                                deng   9.077468   0.000000\n",
       "22                                         sugdensmith   9.040585   0.000000\n",
       "23                                    directorminister   8.967411   0.000000\n",
       "24                                             tritium   8.907054   8.907054\n",
       "25                                               saffi   8.842461   8.842461\n",
       "26                                           ebayclark   8.698475   0.000000\n",
       "27                                          systemwhen   8.421546   0.000000\n",
       "28                                             playsli   8.281231   0.000000\n",
       "29                                              mrtons   7.712676   5.141784\n",
       "..                                                 ...        ...        ...\n",
       "378                                               50cc   0.000000   2.660120\n",
       "379                                            convery   0.000000   2.527567\n",
       "380                                       wkiewdekwrza   0.000000   2.496468\n",
       "381                                        ranthambore   0.000000   2.453823\n",
       "382                                        himswannack   0.000000   2.449107\n",
       "383                                            previte   0.000000   2.440865\n",
       "384                                              clyde   0.000000   2.375084\n",
       "385                                       chearavanont   0.000000   2.343022\n",
       "386                                        adrenalinin   0.000000   2.339854\n",
       "387                                           sivarasa   0.000000   2.299053\n",
       "388                                               5153   0.000000   2.240256\n",
       "389                                       princeweaver   0.000000   2.166978\n",
       "390                                       thoroughfare   0.000000   2.144758\n",
       "391                                         exuberance   0.000000   1.960920\n",
       "392                                  philippinesdisney   0.000000   1.929423\n",
       "393                                        courtsother   0.000000   1.875313\n",
       "394                                            33story   0.000000   1.848403\n",
       "395                                           dieguito   0.000000   1.759101\n",
       "396                                 homeownergardeners   0.000000   1.673057\n",
       "397                                          gameknown   0.000000   1.545440\n",
       "398                                      awardyearwood   0.000000   1.521978\n",
       "399                                            cechova   0.000000   1.429450\n",
       "400                                             spanky   0.000000   1.418620\n",
       "401                                        alliancesan   0.000000   1.406248\n",
       "402                                           farmerhe   0.000000   1.357313\n",
       "403                                              fuine   0.000000   1.325334\n",
       "404                                         rolesrivka   0.000000   1.313995\n",
       "405                                          1973davis   0.000000   0.977477\n",
       "406                                                gap   0.000000   0.462727\n",
       "407  hangshttpwwwmoroccoworldnewscom201410141065rab...   0.000000   0.298239\n",
       "\n",
       "[408 rows x 3 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_tf_idf = top_words_tf_idf('Barack Obama')\n",
    "biden_tf_idf = top_words_tf_idf('Joe Biden')\n",
    "\n",
    "combined_words = pd.merge(left= obama_tf_idf , right = biden_tf_idf, \n",
    "                          how= 'outer', left_on='word', right_on='word')\n",
    "combined_words.columns= ['word','Obama','Biden']\n",
    "combined_words = combined_words.fillna(0)\n",
    "combined_words.head()\n",
    "combined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 123.29745601]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "print( pairwise_distances ( combined_words['Obama'].values.reshape(1,-1), \n",
    "                           combined_words['Biden'].values.reshape(1,-1),\n",
    "                           metric='euclidean') )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance is larger than the distances we found for the 10 nearest neighbors, which we repeat here for readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phil Schiliro</td>\n",
       "      <td>106.861014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeff Sessions</td>\n",
       "      <td>108.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jesse Lee (politician)</td>\n",
       "      <td>109.045698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samantha Power</td>\n",
       "      <td>109.108106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bob Menendez</td>\n",
       "      <td>109.781867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eric Stern (politician)</td>\n",
       "      <td>109.957788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>James A. Guest</td>\n",
       "      <td>110.413889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Roland Grossenbacher</td>\n",
       "      <td>110.470609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tulsi Gabbard</td>\n",
       "      <td>110.696998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Howard Dawson</td>\n",
       "      <td>110.730547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name           0\n",
       "0              Barack Obama    0.000000\n",
       "1             Phil Schiliro  106.861014\n",
       "2             Jeff Sessions  108.871674\n",
       "3    Jesse Lee (politician)  109.045698\n",
       "4            Samantha Power  109.108106\n",
       "5              Bob Menendez  109.781867\n",
       "6   Eric Stern (politician)  109.957788\n",
       "7            James A. Guest  110.413889\n",
       "8      Roland Grossenbacher  110.470609\n",
       "9             Tulsi Gabbard  110.696998\n",
       "10            Howard Dawson  110.730547"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OB_index = wiki[wiki['name']== 'Barack Obama'].index.tolist()[0]\n",
    "\n",
    "model_tf_idf =  NearestNeighbors(metric='euclidean', algorithm='brute')\n",
    "model_tf_idf.fit(tf_idf) \n",
    "\n",
    "distance2, index2 = model_tf_idf.kneighbors(X = tf_idf[OB_index], \n",
    "                                            n_neighbors=11) \n",
    "\n",
    "pd.concat([wiki[['name']].iloc[index2[0].tolist()].reset_index().drop('index', axis=1) ,\n",
    "           pd.Series(distance2[0])] , axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But one may wonder, is Biden's article that different from Obama's, more so than, say, Schiliro's? It turns out that, when we compute nearest neighbors using the Euclidean distances, we unwittingly favor short articles over long ones. Let us compute the length of each Wikipedia document, and examine the document lengths for the 100 nearest neighbors to Obama's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_length(row):\n",
    "    return len(row['text'].split(' '))\n",
    "\n",
    "wiki['length'] = wiki.apply(compute_length, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NearestNeighbors' object has no attribute 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-ae99bc7dd4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnearest_neighbors_euclidean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_tf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Barack Obama'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnearest_neighbors_euclidean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnearest_neighbors_euclidean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'reference_label'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NearestNeighbors' object has no attribute 'query'"
     ]
    }
   ],
   "source": [
    "nearest_neighbors_euclidean = model_tf_idf.query(wiki[wiki['name'] == 'Barack Obama'], label='name', k=100)\n",
    "nearest_neighbors_euclidean = nearest_neighbors_euclidean.join(wiki[['name', 'length']], on={'reference_label':'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_euclidean.sort('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how these document lengths compare to the lengths of other documents in the corpus, let's make a histogram of the document lengths of Obama's 100 nearest neighbors and compare to a histogram of document lengths for all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.hist(wiki['length'], 50, color='k', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='Entire Wikipedia', zorder=3, alpha=0.8)\n",
    "plt.hist(nearest_neighbors_euclidean['length'], 50, color='r', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (Euclidean)', zorder=10, alpha=0.8)\n",
    "plt.axvline(x=wiki['length'][wiki['name'] == 'Barack Obama'][0], color='k', linestyle='--', linewidth=4,\n",
    "           label='Length of Barack Obama', zorder=2)\n",
    "plt.axvline(x=wiki['length'][wiki['name'] == 'Joe Biden'][0], color='g', linestyle='--', linewidth=4,\n",
    "           label='Length of Joe Biden', zorder=1)\n",
    "plt.axis([0, 1000, 0, 0.04])\n",
    "\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.title('Distribution of document length')\n",
    "plt.xlabel('# of words')\n",
    "plt.ylabel('Percentage')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to the rest of Wikipedia, nearest neighbors of Obama are overwhemingly short, most of them being shorter than 300 words. The bias towards short articles is not appropriate in this application as there is really no reason to  favor short articles over long articles (they are all Wikipedia articles, after all). Many of the Wikipedia articles are 300 words or more, and both Obama and Biden are over 300 words long.\n",
    "\n",
    "**Note**: For the interest of computation time, the dataset given here contains _excerpts_ of the articles rather than full text. For instance, the actual Wikipedia article about Obama is around 25000 words. Do not be surprised by the low numbers shown in the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Both word-count features and TF-IDF are proportional to word frequencies. While TF-IDF penalizes very common words, longer articles tend to have longer TF-IDF vectors simply because they have more words in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To remove this bias, we turn to **cosine distances**:\n",
    "$$\n",
    "d(\\mathbf{x},\\mathbf{y}) = 1 - \\frac{\\mathbf{x}^T\\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}\n",
    "$$\n",
    "Cosine distances let us compare word distributions of two articles of varying lengths.\n",
    "\n",
    "Let us train a new nearest neighbor model, this time with cosine distances.  We then repeat the search for Obama's 100 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_tf_idf = graphlab.nearest_neighbors.create(wiki, label='name', features=['tf_idf'],\n",
    "                                                  method='brute_force', distance='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_cosine = model2_tf_idf.query(wiki[wiki['name'] == 'Barack Obama'], label='name', k=100)\n",
    "nearest_neighbors_cosine = nearest_neighbors_cosine.join(wiki[['name', 'length']], on={'reference_label':'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_neighbors_cosine.sort('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a glance at the above table, things look better.  For example, we now see Joe Biden as Barack Obama's nearest neighbor!  We also see Hillary Clinton on the list.  This list looks even more plausible as nearest neighbors of Barack Obama.\n",
    "\n",
    "Let's make a plot to better visualize the effect of having used cosine distance in place of Euclidean on our TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.figure(figsize=(10.5,4.5))\n",
    "plt.hist(wiki['length'], 50, color='k', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='Entire Wikipedia', zorder=3, alpha=0.8)\n",
    "plt.hist(nearest_neighbors_euclidean['length'], 50, color='r', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (Euclidean)', zorder=10, alpha=0.8)\n",
    "plt.hist(nearest_neighbors_cosine['length'], 50, color='b', edgecolor='None', histtype='stepfilled', normed=True,\n",
    "         label='100 NNs of Obama (cosine)', zorder=11, alpha=0.8)\n",
    "plt.axvline(x=wiki['length'][wiki['name'] == 'Barack Obama'][0], color='k', linestyle='--', linewidth=4,\n",
    "           label='Length of Barack Obama', zorder=2)\n",
    "plt.axvline(x=wiki['length'][wiki['name'] == 'Joe Biden'][0], color='g', linestyle='--', linewidth=4,\n",
    "           label='Length of Joe Biden', zorder=1)\n",
    "plt.axis([0, 1000, 0, 0.04])\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.title('Distribution of document length')\n",
    "plt.xlabel('# of words')\n",
    "plt.ylabel('Percentage')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the 100 nearest neighbors using cosine distance provide a sampling across the range of document lengths, rather than just short articles like Euclidean distance provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moral of the story**: In deciding the features and distance measures, check if they produce results that make sense for your particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem with cosine distances: tweets vs. long articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily ever after? Not so fast. Cosine distances ignore all document lengths, which may be great in certain situations but not in others. For instance, consider the following (admittedly contrived) example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+--------------------------------------------------------+\n",
    "|                                             +--------+ |\n",
    "|  One that shall not be named                | Follow | |\n",
    "|  @username                                  +--------+ |\n",
    "|                                                        |\n",
    "|  Democratic governments control law in response to     |\n",
    "|  popular act.                                          |\n",
    "|                                                        |\n",
    "|  8:05 AM - 16 May 2016                                 |\n",
    "|                                                        |\n",
    "|  Reply   Retweet (1,332)   Like (300)                  |\n",
    "|                                                        |\n",
    "+--------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar is this tweet to Barack Obama's Wikipedia article? Let's transform the tweet into TF-IDF features, using an encoder fit to the Wikipedia dataset.  (That is, let's treat this tweet as an article in our Wikipedia dataset and see what happens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf = graphlab.SFrame({'text': ['democratic governments control law in response to popular act']})\n",
    "sf['word_count'] = graphlab.text_analytics.count_words(sf['text'])\n",
    "\n",
    "encoder = graphlab.feature_engineering.TFIDF(features=['word_count'], output_column_prefix='tf_idf')\n",
    "encoder.fit(wiki)\n",
    "sf = encoder.transform(sf)\n",
    "sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the TF-IDF vectors for this tweet and for Barack Obama's Wikipedia entry, just to visually see their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_tf_idf = sf[0]['tf_idf.word_count']\n",
    "tweet_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obama = wiki[wiki['name'] == 'Barack Obama']\n",
    "obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the cosine distance between the Barack Obama article and this tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obama_tf_idf = obama[0]['tf_idf']\n",
    "graphlab.toolkits.distances.cosine(obama_tf_idf, tweet_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this distance to the distance between the Barack Obama article and all of its Wikipedia 10 nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_tf_idf.query(obama, label='name', k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cosine distances, the tweet is \"nearer\" to Barack Obama than everyone else, except for Joe Biden!  This probably is not something we want. If someone is reading the Barack Obama Wikipedia page, would you want to recommend they read this tweet? Ignoring article lengths completely resulted in nonsensical results. In practice, it is common to enforce maximum or minimum document lengths. After all, when someone is reading a long article from _The Atlantic_, you wouldn't recommend him/her a tweet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
